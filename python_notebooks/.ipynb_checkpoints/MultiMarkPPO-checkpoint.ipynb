{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc0f92c-b219-45eb-9cf1-e2671e6581dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from gymnasium import spaces, Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86dba737-4815-472e-b49d-f29cc60ac807",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiMarkEnv(Env):\n",
    "    def __init__(self, config):\n",
    "        self.min_x = -400\n",
    "        self.max_x = 400\n",
    "        self.min_y = -400\n",
    "        self.max_y = 400\n",
    "        self.MAX_SPEED = 10\n",
    "        self.TURNING_RATE = 12.5 * 180 / np.pi\n",
    "        self.ITERS_PER_ACTION = 5\n",
    "        self.SPEED_PENALTY = 0.4\n",
    "        self.SPEED_RECOVERY_IN_SECONDS = 4\n",
    "        self.MAX_REMAINING_SECONDS = config['max_remaining_seconds']\n",
    "        self.target_x = 0\n",
    "        self.target_y = 0\n",
    "\n",
    "        self.MAX_DISTANCE = np.sqrt((self.max_x - self.min_x) ** 2 + (self.max_y - self.min_y) ** 2)\n",
    "\n",
    "        self.coords = config['coords']\n",
    "        \n",
    "        self.observation = np.zeros((6,))\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(1,))\n",
    "        \n",
    "        # fields = [\n",
    "        #   :distance,\n",
    "        #   :vmg,\n",
    "        #   :heading,\n",
    "        #   :angle_to_target,\n",
    "        #   :has_tacked,\n",
    "        #   :has_reached_target\n",
    "        # ]\n",
    "        \n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,))\n",
    "        self.reward_range = spaces.Box(low=-1, high=self.MAX_SPEED, shape=())\n",
    "\n",
    "    def reset(self, seed = None, options = None):\n",
    "        super().reset(seed=seed)\n",
    "        # Initialization logic\n",
    "        # Initialize state variables: x, y, speed, etc.\n",
    "        # Return the initial observation\n",
    "\n",
    "        self.tack_count = 0\n",
    "        self.heading = 0\n",
    "        self.angle_to_mark = 0\n",
    "        self.speed = 0\n",
    "        self.vmg = 0\n",
    "        self.has_tacked = False\n",
    "        self.remaining_seconds = self.MAX_REMAINING_SECONDS\n",
    "        self.delta_t = 0\n",
    "\n",
    "        random_observation = np.zeros(self.observation_space.shape)  \n",
    "\n",
    "        coord_idx = np.random.choice(np.arange(0, self.coords.shape[0]))\n",
    "        coord = self.coords[coord_idx]\n",
    "\n",
    "        self.x = coord[0]\n",
    "        self.y = coord[1]\n",
    "        self.target_x = coord[2]\n",
    "        self.target_y = coord[3]\n",
    "        self.distance = np.sqrt((self.target_x - self.x) ** 2 + self.target_y ** 2)\n",
    "        self.initial_distance = self.distance\n",
    "\n",
    "        self.observation[0] = self.distance / self.MAX_DISTANCE\n",
    "        \n",
    "        return self.observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.apply_action(action).calculate_reward().is_terminal()\n",
    "\n",
    "        self.observation = np.stack([\n",
    "            self.distance / self.MAX_DISTANCE,\n",
    "            self.vmg,\n",
    "            self.heading,\n",
    "            self.angle_to_mark,\n",
    "            self.has_tacked,\n",
    "            self.is_terminal and not self.is_truncated\n",
    "        ])\n",
    "        \n",
    "        return self.observation, self.reward, self.is_terminal, self.is_truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def apply_action(self, action):\n",
    "        dtheta = action * np.pi\n",
    "        prev_heading = self.heading\n",
    "        \n",
    "        turning_time = np.abs(dtheta) / self.TURNING_RATE\n",
    "        dt = turning_time / self.ITERS_PER_ACTION\n",
    "        \n",
    "        dtheta_steps = dt * np.sign(dtheta) * np.broadcast_to(self.TURNING_RATE, (self.ITERS_PER_ACTION))\n",
    "        heading_steps = np.cumsum(dtheta_steps) + prev_heading\n",
    "        heading_steps = self.wrap_phase(heading_steps)\n",
    "\n",
    "        speed_steps = self.speed_from_heading(heading_steps)\n",
    "\n",
    "        tacking_mask = heading_steps < np.pi != prev_heading < np.pi\n",
    "        # cumulative max for boolean:\n",
    "        tacking_mask = np.cumsum(tacking_mask) > 0\n",
    "\n",
    "        speed_penalty_multiplier = 1 - self.SPEED_PENALTY\n",
    "\n",
    "        penalized_speed_steps = np.where(tacking_mask, speed_penalty_multiplier * speed_steps, speed_steps)\n",
    "        has_tacked = np.any(tacking_mask)\n",
    "        tack_count = self.tack_count + has_tacked\n",
    "\n",
    "        dy = dt * np.cos(heading_steps) * penalized_speed_steps\n",
    "        dx = dt * np.sin(heading_steps) * penalized_speed_steps\n",
    "\n",
    "        x = self.x + np.sum(dx)\n",
    "        y = self.y + np.sum(dy)\n",
    "\n",
    "        heading = heading_steps[-1]\n",
    "        speed = speed_steps[-1]\n",
    "        \n",
    "        speed_steps = np.linspace(speed, speed / speed_penalty_multiplier, num=self.SPEED_RECOVERY_IN_SECONDS)\n",
    "        x = x + np.sin(heading) * np.sum(speed_steps)\n",
    "        y = y + np.cos(heading) * np.sum(speed_steps)\n",
    "        speed = speed_steps[-1]\n",
    "\n",
    "        dx = self.target_x - x\n",
    "        dy = self.target_y - y\n",
    "\n",
    "        angle_to_mark = self.wrap_phase(np.atan2(dx, dy))\n",
    "\n",
    "        target_unit = np.stack([np.cos(angle_to_mark), np.sin(angle_to_mark)])\n",
    "        heading_unit = np.stack([np.cos(heading), np.sin(heading)])\n",
    "\n",
    "        vmg = (target_unit @ heading_unit) * speed\n",
    "        delta_t = turning_time + self.SPEED_RECOVERY_IN_SECONDS\n",
    "\n",
    "        distance = np.sqrt((self.target_x - x) ** 2 + (self.target_y - y) ** 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.tack_count = tack_count\n",
    "        self.heading = heading\n",
    "        self.angle_to_mark = angle_to_mark\n",
    "        self.speed = speed\n",
    "        self.vmg = vmg\n",
    "        self.has_tacked = has_tacked\n",
    "        self.distance = distance\n",
    "        self.remaining_seconds -= delta_t\n",
    "        self.delta_t = delta_t\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def wrap_phase(self, angles):\n",
    "        return np.remainder(np.remainder(angles, 2 * np.pi) + 2 * np.pi, 2 * np.pi)\n",
    "\n",
    "    def speed_from_heading(self, headings):\n",
    "        raise \"not implemented\"\n",
    "\n",
    "    def is_terminal(self):\n",
    "        if self.distance < 20:\n",
    "            self.is_terminal = True\n",
    "            self.is_truncated = False\n",
    "            return self\n",
    "\n",
    "        has_collided = self.x < self.MIN_X or self.x > self.MAX_X or self.y < self.MIN_Y or self.y > self.MAX_Y\n",
    "        \n",
    "        if has_collided or self.remaining_seconds < 1:\n",
    "            self.is_terminal = True\n",
    "            self.is_truncated = True\n",
    "            return self\n",
    "\n",
    "        self.is_terminal = False\n",
    "        self.is_truncated = False\n",
    "        return self\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        if self.is_terminal and not self.is_truncated:\n",
    "            # good ending\n",
    "            self.reward = 1.0\n",
    "            return self\n",
    "\n",
    "        if self.is_terminal and self.is_truncated:\n",
    "            self.reward = -self.distance / self.initial_distance\n",
    "            return self\n",
    "\n",
    "        if self.has_tacked:\n",
    "            self.reward = -0.1\n",
    "            return self\n",
    "\n",
    "        self.reward = -0.01 * self.vmg / self.MAX_SPEED\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "123abf43-2501-463f-a35c-cf5455a9ddb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "exceptions must derive from BaseException",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.5/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.5/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.5/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[1;32m    176\u001b[0m     clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.5/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.5/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.5/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "Cell \u001b[0;32mIn[5], line 67\u001b[0m, in \u001b[0;36mMultiMarkEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcalculate_reward()\u001b[38;5;241m.\u001b[39mis_terminal()\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_DISTANCE,\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvmg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_terminal \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_truncated\n\u001b[1;32m     76\u001b[0m     ])\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_terminal, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_truncated, {}\n",
      "Cell \u001b[0;32mIn[5], line 94\u001b[0m, in \u001b[0;36mMultiMarkEnv.apply_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     91\u001b[0m heading_steps \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcumsum(dtheta_steps) \u001b[38;5;241m+\u001b[39m prev_heading\n\u001b[1;32m     92\u001b[0m heading_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_phase(heading_steps)\n\u001b[0;32m---> 94\u001b[0m speed_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeed_from_heading\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheading_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m tacking_mask \u001b[38;5;241m=\u001b[39m heading_steps \u001b[38;5;241m<\u001b[39m np\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m!=\u001b[39m prev_heading \u001b[38;5;241m<\u001b[39m np\u001b[38;5;241m.\u001b[39mpi\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# cumulative max for boolean:\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 151\u001b[0m, in \u001b[0;36mMultiMarkEnv.speed_from_heading\u001b[0;34m(self, headings)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspeed_from_heading\u001b[39m(\u001b[38;5;28mself\u001b[39m, headings):\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: exceptions must derive from BaseException"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "\n",
    "r = 250\n",
    "\n",
    "config = {\n",
    "    'max_tacks': 2,\n",
    "    'coords': np.array([[0, 0, 0, r], [0, r, 0, 0]]),\n",
    "    'max_remaining_seconds': 500\n",
    "}\n",
    "\n",
    "# Initialize Environment\n",
    "env = MultiMarkEnv(config)\n",
    "\n",
    "# Initialize PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02522338-6ed7-495b-9901-9550866e1754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
