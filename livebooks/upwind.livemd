# Upwind Mark Training

```elixir
my_app_root = Path.join(__DIR__, "..")

Mix.install(
  [
    {:boat_learner, path: my_app_root, env: :dev}
  ],
  config_path: Path.join(my_app_root, "config/config.exs"),
  lockfile: Path.join(my_app_root, "mix.lock"),
  # change to "cuda118" to use CUDA
  system_env: %{"XLA_TARGET" => "cpu"}
)
```

## Plot

```elixir
alias VegaLite, as: Vl

{min_x, max_x, min_y, max_y} = BoatLearner.Environments.UpwindMark.bounding_box()

possible_targets_l = [[0, 30]]

possible_targets = Nx.tensor(Enum.shuffle(possible_targets_l))

width = 600
height = 600

angle_templates_base = [%{x: -25, y: 34.72}, %{x: 0, y: 5}, %{x: 25, y: 34.72}]

angle_templates =
  Enum.flat_map(0..3, fn offset ->
    Enum.map(angle_templates_base, fn %{x: x, y: y} ->
      y = y + 10 * offset

      %{x: x, y: y, group: offset}
    end)
  end)

max_y = max(max_y, angle_templates |> Enum.map(& &1.y) |> Enum.max())

grid_widget =
  Vl.new(width: width, height: height)
  |> Vl.layers([
    Vl.new()
    |> Vl.data_from_values(angle_templates, name: "angles")
    |> Vl.mark(:line, opacity: 0.25)
    |> Vl.encode_field(:x, "x", type: :quantitative, scale: [clamp: false])
    |> Vl.encode_field(:y, "y", type: :quantitative, scale: [clamp: false])
    |> Vl.encode_field(:color, "group", type: :nominal, scale: [scheme: "blues"], legend: false),
    Vl.new()
    |> Vl.data(name: "target")
    |> Vl.mark(:point,
      fill: true,
      tooltip: [content: "data"],
      grid: true,
      size: [expr: "height * 4 * #{:math.pi()} / #{max_y - min_y}"]
    )
    |> Vl.encode_field(:x, "x", type: :quantitative)
    |> Vl.encode_field(:y, "y", type: :quantitative)
    |> Vl.encode_field(:color, "episode",
      type: :nominal,
      scale: [scheme: "blues"],
      legend: false
    ),
    Vl.new()
    |> Vl.data(name: "trajectory")
    |> Vl.mark(:line, opacity: 1, tooltip: [content: "data"])
    |> Vl.encode_field(:x, "x", type: :quantitative, scale: [domain: [min_x, max_x], clamp: true])
    |> Vl.encode_field(:y, "y", type: :quantitative, scale: [domain: [min_y, max_y], clamp: true])
    |> Vl.encode_field(:color, "episode",
      type: :nominal,
      scale: [scheme: "blues"],
      legend: false
    )
    |> Vl.encode_field(:order, "index")
  ])
  |> Kino.VegaLite.new()
  |> Kino.render()

loss_widget =
  Vl.new(width: width, height: div(height, 2), title: "Loss")
  |> Vl.data(name: "loss")
  |> Vl.mark(:line,
    grid: true,
    tooltip: [content: "data"],
    interpolate: "step-after",
    point: true,
    color: "blue"
  )
  |> Vl.encode_field(:x, "episode", type: :quantitative)
  |> Vl.encode_field(:y, "loss",
    type: :quantitative,
    scale: [
      domain: [0, 32],
      type: "symlog",
      base: 2,
      clamp: true
    ],
    axis: [tick_min_step: 1]
  )
  |> Vl.encode_field(:order, "episode")
  |> Kino.VegaLite.new()
  |> Kino.render()

reward_widget =
  Vl.new(width: width, height: div(height, 2), title: "Total Reward per Epoch")
  |> Vl.data(name: "reward")
  |> Vl.mark(:line,
    grid: true,
    tooltip: [content: "data"],
    interpolate: "step-after",
    point: true,
    color: "blue"
  )
  |> Vl.encode_field(:x, "episode", type: :quantitative)
  |> Vl.encode_field(:y, "reward",
    type: :quantitative,
    scale: [
      domain: [-1.5e3, 1.5e3],
      type: "symlog",
      base: 10,
      clamp: true
    ]
  )
  |> Vl.encode_field(:order, "episode")
  |> Kino.VegaLite.new()
  |> Kino.render()

nil
```

```elixir
defmodule AccumulateAndExportData do
  use GenServer

  def state_to_trajectory_entry(%{environment_state: env}) do
    target_vector = Nx.complex(Nx.subtract(env.target_x, env.x), Nx.subtract(env.target_y, env.y))
    distance_to_mark = Nx.abs(target_vector)
    angle_to_mark = Nx.phase(target_vector)

    vmg =
      BoatLearner.Environments.UpwindMark.vmg(
        env.x,
        env.y,
        env.target_x,
        env.target_y,
        env.angle,
        env.speed
      )

    [
      env.x,
      env.y,
      env.angle,
      env.speed,
      vmg,
      angle_to_mark,
      distance_to_mark
    ]
  end

  def init(_opts), do: {:ok, %{trajectories: [], marks: []}}

  def start_link(opts \\ []), do: GenServer.start_link(__MODULE__, opts, name: __MODULE__)

  def reset, do: GenServer.cast(__MODULE__, :reset)

  def handle_cast(:reset, _) do
    {:ok, state} = init([])
    {:noreply, state}
  end

  def handle_cast({:add_epoch, epoch, target_x, target_y, iterations, trajectory_tensor}, state) do
    trajectory_data =
      trajectory_tensor
      |> Nx.slice_along_axis(0, iterations, axis: 0)
      |> Nx.to_list()
      |> Enum.map(fn [
                       x,
                       y,
                       angle,
                       speed,
                       vmg,
                       angle_to_mark,
                       distance_to_mark
                     ] ->
        %{
          x: x,
          y: y,
          epoch: epoch,
          angle_of_attack: angle,
          boat_speed: speed,
          angle_to_mark: angle_to_mark,
          distance_to_mark: distance_to_mark,
          vmg: vmg
        }
      end)

    mark = %{x: target_x, y: target_y, epoch: epoch}

    %{trajectory: [trajectory_data | state.trajectory], marks: [mark | state.marks]}
  end

  def handle_call({:save, filename}, _from, state) do
    layers = [
      %{
        mark: :point,
        opts: [fill: true, tooltip: [content: "data"], grid: true, size: 50],
        data: Enum.reverse(state.marks)
      },
      %{
        mark: %{type: :line, opts: [tooltip: [content: "data"]]},
        data: state.trajectories |> Enum.reverse() |> List.flatten()
      }
    ]

    contents = :erlang.term_to_binary(layers)
    File.write!(filename, layers)
  end
end
```

```elixir
# 250 max_iter * 15 episodes
max_points = 1000

plot_fn = fn axon_state ->
  episode = axon_state.epoch
  IO.inspect("Episode #{episode} ended")

  if axon_state.iteration > 1 and rem(episode, 10) == 0 do
    Kino.VegaLite.clear(grid_widget, dataset: "target")
    Kino.VegaLite.clear(grid_widget, dataset: "trajectory")

    target_x = Nx.to_number(axon_state.step_state.environment_state.target_x)
    target_y = Nx.to_number(axon_state.step_state.environment_state.target_y)

    Kino.VegaLite.push(
      grid_widget,
      %{
        x: target_x,
        y: target_y,
        episode: episode
      },
      dataset: "target"
    )

    trajectory = axon_state.step_state.trajectory

    GenServer.cast(
      AccumulateAndExportData,
      {:add_epoch, episode, target_x, target_y, axon_state.iteration, trajectory}
    )

    idx = Enum.to_list(0..(axon_state.iteration - 1)//1)
    x = Nx.to_flat_list(trajectory[0..(axon_state.iteration - 1)//1][0])
    y = Nx.to_flat_list(trajectory[0..(axon_state.iteration - 1)//1][1])

    points =
      [idx, x, y]
      |> Enum.zip_with(fn [index, x, y] ->
        %{
          x: x,
          y: y,
          index: index,
          episode: episode
        }
      end)

    Kino.VegaLite.push_many(grid_widget, points, dataset: "trajectory")

    Kino.VegaLite.push(
      loss_widget,
      %{
        episode: episode,
        loss: Nx.to_number(axon_state.step_state.agent_state.loss)
      },
      dataset: "loss"
    )

    Kino.VegaLite.push(
      reward_widget,
      %{
        episode: axon_state.epoch,
        reward: Nx.to_number(axon_state.step_state.agent_state.total_reward)
      },
      dataset: "reward"
    )
  end

  axon_state
end
```

```elixir
filename = Path.join(System.fetch_env!("HOME"), "Desktop/upwind1.bin")
export_filename = Path.join(System.fetch_env!("HOME"), "Desktop/upwind1_data.dat")

{
  q_policy,
  experience_replay_buffer_index,
  persisted_experience_replay_buffer_entries,
  experience_replay_buffer,
  total_episodes
} =
  try do
    contents = File.read!(filename)
    File.write!(filename <> "_bak", contents)

    %{serialized: serialized, total_episodes: total_episodes} = :erlang.binary_to_term(contents)

    %{
      q_policy: q_policy,
      experience_replay_buffer_index: experience_replay_buffer_index,
      persisted_experience_replay_buffer_entries: persisted_experience_replay_buffer_entries,
      experience_replay_buffer: exp_replay_buffer
    } = Nx.deserialize(serialized)

    {q_policy, experience_replay_buffer_index, persisted_experience_replay_buffer_entries,
     exp_replay_buffer, total_episodes}
  rescue
    File.Error ->
      {%{}, 0, 0, nil, 0}
  end

# q_policy = %{}
# total_episodes = 0
# experience_replay_buffer = nil
# persisted_experience_replay_buffer_entries = 0
# experience_replay_buffer_index = 0
total_episodes
```

```elixir
q_policy
```

```elixir
fields = [
  :x,
  :y,
  :speed,
  :target_x,
  :target_y,
  :fuel
]

num_actions = BoatLearner.Environments.UpwindMark.num_actions()
num_observations = length(fields)

policy_net =
  Axon.input("state", shape: {nil, num_observations})
  |> Axon.dense(128, activation: :relu)
  |> Axon.dropout(rate: 0.25)
  |> Axon.dense(64, activation: :relu)
  |> Axon.dropout(rate: 0.25)
  |> Axon.dense(64, activation: :relu)
  |> Axon.dense(num_actions)

# These might seem redundant, but will make more sense for multi-input models
normalize_x = fn x ->
  import Kernel, only: []
  import Nx.Defn.Kernel, only: [-: 2, /: 2]
  (x - min_x) / (max_x - min_x)
end

normalize_y = fn y ->
  import Kernel, only: []
  import Nx.Defn.Kernel, only: [-: 2, /: 2]
  (y - min_y) / (max_y - min_y)
end

normalize_speed = fn s ->
  import Kernel, only: []
  import Nx.Defn.Kernel, only: [-: 2, +: 2, /: 2]
  s / (max_y - min_y + max_x - min_x)
end

normalize_angle = fn a ->
  import Kernel, only: []
  import Nx.Defn.Kernel, only: [*: 2, /: 2]
  a / (2 * :math.pi())
end

environment_to_state_vector_fn = fn env_state ->
  [
    normalize_x.(env_state.x),
    normalize_y.(env_state.y),
    normalize_x.(env_state.target_x),
    normalize_y.(env_state.target_y),
    normalize_speed.(env_state.speed),
    Nx.divide(env_state.fuel, env_state.max_fuel)
  ]
  |> Nx.stack()
  |> Nx.new_axis(0)
end

environment_to_input_fn = fn env_state ->
  %{"state" => environment_to_state_vector_fn.(env_state)}
end

state_vector_to_input_fn = fn state_vector ->
  %{"state" => state_vector}
end
```

## Train

```elixir
Kino.VegaLite.clear(grid_widget)
Kino.VegaLite.clear(loss_widget, dataset: "loss")
Kino.VegaLite.clear(reward_widget, dataset: "reward")

episodes = 10
max_iter = 250

AccumulateAndExportData.start_link([])
AccumulateAndExportData.reset()

{t, result} =
  :timer.tc(fn ->
    ReinforcementLearning.train(
      {BoatLearner.Environments.UpwindMark,
       max_fuel: max_iter, possible_targets: possible_targets},
      {ReinforcementLearning.Agents.DQN,
       policy_net: policy_net,
       eps_max_iter: -1,
       q_policy: q_policy,
       persisted_experience_replay_buffer_entries: persisted_experience_replay_buffer_entries,
       experience_replay_buffer_index: experience_replay_buffer_index,
       experience_replay_buffer: experience_replay_buffer,
       environment_to_input_fn: environment_to_input_fn,
       environment_to_state_vector_fn: environment_to_state_vector_fn,
       state_vector_to_input_fn: state_vector_to_input_fn},
      plot_fn,
      &AccumulateAndExportData.state_to_trajectory_entry/1,
      num_episodes: episodes,
      max_iter: max_iter
    )
  end)

GenServer.call(AccumulateAndExportData, {:save, export_filename})

serialized =
  Nx.serialize(
    Map.take(result.step_state.agent_state, [
      :q_policy,
      :experience_replay_buffer_index,
      :experience_replay_buffer,
      :persisted_experience_replay_buffer_entries
    ])
  )

contents =
  :erlang.term_to_binary(%{serialized: serialized, total_episodes: total_episodes + episodes})

"#{Float.round(t / 1_000_000, 3)} s"
```

```elixir
File.write!(filename, contents)
result.step_state.agent_state.q_policy
```
